import torch
import torch.nn as nn
import random
import string
from constants import SOS_TOKEN, EOS_TOKEN
from .sampling import GreedySampler, RandomSampler, BeamSearch
# [New] util에서 init_weights 임포트
from util import init_weights 

class Seq2SeqTrain(nn.Module):
    def __init__(self, encoder, decoder, vocab_size, teacher_forcing_ratio=0.5):
        """
        [Docstring 유지]
        """
        super(Seq2SeqTrain, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.vocab_size = vocab_size
        self.teacher_forcing_ratio = teacher_forcing_ratio
        
        # [New] 논문 재현을 위한 파라미터 초기화 (Uniform -0.1 ~ 0.1)
        # Encoder, Decoder를 포함한 전체 서브모듈에 적용
        self.apply(init_weights)

    def forward(self, question, answer):
        # [기존 코드 유지]
        answer_seq_len = answer.size(0)
        batch_size = answer.size(1)
        
        encoder_outputs, h_n = self.encoder(question)
        
        # Explicit slicing for decoder input and labels
        # answer: (tgt_len, batch)
        decoder_input = answer[:-1]   # (tgt_len-1, batch)  <sos> ... 마지막 단어
        target_label = answer[1:]     # (tgt_len-1, batch)  첫 단어 ... <eos>
        
        # Pre-allocate output tensor to avoid repeated concatenation
        # Shape: (seq_len-1, batch_size, vocab_size)
        # Use encoder_outputs dtype to match model precision (supports mixed precision)
        #outputs = torch.empty(answer_seq_len - 1, batch_size, self.vocab_size,
        #                    dtype=encoder_outputs.dtype, device=answer.device)
        outputs = torch.empty(
            decoder_input.size(0), batch_size, self.vocab_size,
            dtype=encoder_outputs.dtype, device=answer.device
        )
        
        # Handle edge case: if decoder_input is empty (answer only contains <sos>)
        if decoder_input.size(0) == 0:
            return outputs
        
        kwargs = {}
        input_word = answer[0]  
        input_word = decoder_input[0]
        #for t in range(answer_seq_len - 1):
        for t in range(decoder_input.size(0)):
            output, attn_weights, kwargs = self.decoder(t, input_word, encoder_outputs, h_n, **kwargs)
            
            # Fill pre-allocated tensor in-place (no concatenation needed)
            outputs[t] = output

            teacher_forcing = random.random() < self.teacher_forcing_ratio
            if teacher_forcing:
                #input_word = answer[t + 1]  
                input_word = target_label[t]
            else:
                _, argmax = output.max(dim=1)
                input_word = argmax  

        return outputs

class Seq2SeqPredict(nn.Module):
    """
    This class is wrapper around pre-trained model which can be used for testing model.
    This model takes (numericalized) input, delegates it to appropriate sequence sampler and returns

    :param encoder: Pre-trained encoder.
    :param decoder: Pre-trained decoder.
    :param field: Torchtext Field object which handles processing of raw data into tensors.

    Inputs: questions, sampling_strategy, max_seq_len
        - **questions** list(str): List of raw question strings.
        - **sampling_strategy** (str): Strategy for sampling output sequences. ['greedy', 'random', 'beam_search']
        - **max_seq_len** (scalar): Maximum length of output sequence.

    Outputs: sequences
        - **sequences** list(str): List of answers sequences generated by model.
    """
    def __init__(self, encoder, decoder, field):
        super(Seq2SeqPredict, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.sos_idx = field.vocab.stoi[SOS_TOKEN]
        self.eos_idx = field.vocab.stoi[EOS_TOKEN]
        self.field = field
        self.samplers = {
            'greedy': GreedySampler(),
            'random': RandomSampler(),
            'beam_search': BeamSearch()
        }

    def decode_sequence(self, tokens_idx):
        """
        Decodes token indices to string.

        :param tokens_idx: List of token indices.
        :return: String representing decoded sequence.
        """
        seq = ''
        for idx in tokens_idx:
            tok = self.field.vocab.itos[idx]
            if tok not in string.punctuation and tok[0] != '\'':
                seq += ' '
            seq += tok
        return seq.strip()

    def forward(self, questions, sampling_strategy, max_seq_len):
        # raw strings to tensor
        q = self.field.process([self.field.preprocess(question) for question in questions])
        
        # Move tensor to the same device as the model
        device = next(self.encoder.parameters()).device
        q = q.to(device)

        # encode question sequence
        encoder_outputs, h_n = self.encoder(q)

        # sample output sequence
        sequences, lengths = self.samplers[sampling_strategy].sample(encoder_outputs, h_n, self.decoder, self.sos_idx,
                                                                     self.eos_idx, max_seq_len)

        # torch tensors -> python lists
        batch_size = sequences.size(0)
        sequences, lengths = sequences.tolist(), lengths.tolist()

        # decode output (token idx -> token string)
        seqs = []
        for batch in range(batch_size):
            seq = sequences[batch][:lengths[batch]]
            seqs.append(self.decode_sequence(seq))

        return seqs
